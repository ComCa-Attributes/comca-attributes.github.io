<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="Compositional Caching for Training-free Open-vocabulary Attribute Detection">
    <meta name="keywords" content="ComCa, Compositional Caching, CLIP, VLMs">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Compositional Caching for Training-free Open-vocabulary Attribute Detection</title>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-L7XZ7KC2NP"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-L7XZ7KC2NP');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">Compositional Caching for Training-free Open-vocabulary
                            Attribute Detection</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=BEIogS4AAAAJ">Marco Garosi</a><sup>1</sup>,</span>
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=EPImyCcAAAAJ">Alessandro Conti</a><sup>1</sup>,</span>
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=NIv_aeQAAAAJ">Gaowen Liu</a><sup>2</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=xf1T870AAAAJ">Elisa Ricci</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=bqTPA8kAAAAJ">Massimiliano Mancini</a><sup>1</sup>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>University of Trento,</span>
                            <span class="author-block"><sup>2</sup>Cisco Research</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <!-- <span class="link-block">
                                    <a href="https://arxiv.org/pdf/2011.12948"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span> -->
                                <!-- <span class="link-block">
                                    <a href="#"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span> -->
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/marco-garosi/ComCa"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Teaser. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <div class="content has-text-justified">
                        <img src="./static/images/teaser.png" alt="COPS teaser" />
                        <p>
                            Attribute annotations are often:
                            <span class="tag is-success">sparse</span>, as they are not consistent across samples;
                            <span class="tag is-danger">incomplete</span>, as not all attributes are annotated;
                            <span class="tag is-warning">ambiguous</span>, as they can be subjective or miss a frame of
                            reference.
                            This makes open-vocabulary attribute detection challenging.
                            Differently from previous works, we do not rely on such annotations and propose
                            <b>ComCa</b>, a training-free approach requiring no supervision.
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Teaser. -->
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Attribute detection is crucial for many computer vision tasks, as it enables systems to
                            describe properties such as color, texture, and material.
                        </p>
                        <p>
                            Current approaches often rely on labor-intensive annotation processes which are inherently
                            limited: objects can be described at an arbitrary level of detail (<i>e.g.</i>, color vs.
                            color shades), leading to ambiguities when the annotators are not instructed carefully.
                            Furthermore, they operate within a predefined set of attributes, reducing scalability and
                            adaptability to unforeseen downstream applications.
                        </p>
                        <p>
                            We present <b>Compositional Caching</b> (<b>ComCa</b>), a training-free method for
                            open-vocabulary attribute detection that overcomes these constraints.
                            ComCa requires only the list of target attributes and objects as input, using them to
                            populate an auxiliary cache of images by leveraging web-scale databases and Large Language
                            Models to determine attribute-object compatibility.
                        </p>
                        <p>
                            To account for the compositional nature of attributes, cache images receive soft attribute
                            labels.
                            Those are aggregated at inference time based on the similarity between the input and cache
                            images, refining the predictions of underlying Vision-Language Models (VLMs).
                        </p>
                        <p>
                            Importantly, our approach is <b>model-agnostic</b>, compatible with various VLMs.
                            Experiments on public datasets demonstrate that ComCa significantly outperforms zero-shot
                            and cache-based baselines, competing with recent training-based methods, proving that a
                            carefully designed training-free approach can successfully address open-vocabulary attribute
                            detection.
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Method. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Method</h2>
                    <div class="content has-text-justified">
                        <img src="./static/images/method.png" alt="COPS architecture" />
                        <p>
                            Given a list of <span class="tag is-info">attributes</span> and <span class="tag is-danger">objects</span>, we compute their compatibility from a large <span class="tag is-warning">database</span> \(D_r\) and with an <span class="tag is-warning">LLM</span>.
                            The scores are merged and normalized to obtain the <span class="tag is-success">compatibility distribution</span>, from which we <span class="tag is-warning">sample cache entries</span> ad <span class="tag is-warning">construct the cache</span>.
                            We enrich the latter with soft labels from the VLM-based similarity between cache images and
                            <span class="tag is-info">attributes</span>.
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Method. -->
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Results. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Main results</h2>
                    <div class="content has-text-justified">
                        <img src="./static/images/results.png" alt="Main quantitative results" />
                        <p>
                            <b>Comparison with state of the art.</b>
                            <span class="tag is-success">Green</span> indicates ComCa.
                            <b>Bold</b> indicates best among training-free methods.
                            The symbol "-" indicates results for the competitors are not available on the original papers or the impossibility to run their method due to lack of public code and/or model weights.
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Results. -->
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Qualitative results. -->
            <div class="columns is-centered has-text-justified">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Qualitative results</h2>
                    <div class="content has-text-justified">
                        <img src="./static/images/qualitatives-short.png" alt="Qualitative results" />
                        <p>
                            Predictions of OVAD, CLIP and ComCa on some OVAD images.
                            <span class="tag is-success">Green</span> are correct ones, <span class="tag is-danger">red</span> are wrong.
                        </p>
                    </div>
                    
                    <div class="content has-text-centered">
                        <h3 class="title is-8">Extended qualitative results</h3>
                        <img src="./static/images/qualitatives-1.png"
                            alt="Qualitative results extended - Part 1 of 3" />
                        <p>
                            (a) Comparison of performance on a mobile phone, a tennis racket, and an apple
                        </p>

                        <img src="./static/images/qualitatives-2.png"
                            alt="Qualitative results extended - Part 2 of 3" />
                        <p>
                            (b) Comparison of performance on a cake, a frisbee flying disc, and a PC monitor.
                        </p>

                        <img src="./static/images/qualitatives-3.png"
                            alt="Qualitative results extended - Part 3 of 3" />
                        <p>
                            (c) Comparison of performance on a laptop, a kite, and a skateboard.
                        </p>
                    </div>
                    <p>
                        Top positive predictions of OVAD, CLIP and ComCa on sample images from OVAD.
                        <span class="tag is-success">Green</span> are correct predictions, <span class="tag is-danger">red</span> are wrong ones.
                    </p>
                </div>
            </div>
            <!--/ Qualitative results. -->
        </div>
    </section>


    <section class="section">
        <div class="container is-max-desktop">
            <!-- Concurrent Work. -->
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Related Links</h2>

                    <div class="content has-text-justified">
                        <p>
                            There's a lot of excellent work that ComCa builds upon or compares with. Here are some links, but we refer to the "References" section of our paper for a comprehensive list.
                        </p>
                        <p>
                            <a href="https://arxiv.org/abs/2211.12914">Open-vocabulary Attribute Detection</a>
                            introduces the OVAD benchmark, which we use to evaluate ComCa.
                        </p>
                        <p>
                            <a href="https://arxiv.org/abs/2301.09506">OvarNet: Towards Open-vocabulary Object Attribute Recognition</a>, <a href="https://arxiv.org/abs/2305.20047">LOWA: Localize Objects in the Wild with Attributes</a> and <a href="https://arxiv.org/abs/2408.04102">ArtVLM: Attribute Recognition Through Vision-Based Prefix Language Modeling</a>
                            are all training-based competitors that we compare with in our experiments.
                        </p>
                        <p>
                            We also implement cache-based methods, adapting them to the attribute detection setting. Notably, we compare with <a href="https://arxiv.org/abs/2111.03930">Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling</a> and <a href="https://arxiv.org/abs/2211.16198">SuS-X: Training-Free Name-Only Transfer of Vision-Language Models</a>.
                        </p>
                        <p>
                            Naturally, the backbone of our work is <a href="https://arxiv.org/abs/2103.00020">Learning Transferable Visual Models From Natural Language Supervision</a> (CLIP).
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Concurrent Work. -->
        </div>
    </section>


    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@article{garosi2025comca,
    author  = {Garosi, Marco and Conti, Alessandro and Liu, Gaowen and Ricci, Elisa and Mancini, Massimiliano},
    title   = {Compositional Caching for Training-free Open-vocabulary Attribute Detection},
    journal = {CVPR},
    year    = {2025},
}</code></pre>
        </div>
    </section>


    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <a class="icon-link" href="#">
                    <i class="fas fa-file-pdf"></i>
                </a>
                <a class="icon-link" href="https://github.com/marco-garosi/ComCa" class="external-link" disabled>
                    <i class="fab fa-github"></i>
                </a>
            </div>
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            This website is licensed under a <a rel="license"
                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                                Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>
                        <p>
                            The webpage template is from
                            <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>